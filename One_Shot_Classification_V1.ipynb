{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One-Shot Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Naren-Jegan/Deep-Learning-Keras/blob/master/One_Shot_Classification_V1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "B5KR1KHjuk6N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One Shot Learning on Omniglot Dataset\n",
        "\n",
        "The [Omniglot](https://github.com/brendenlake/omniglot) dataset contains 1623 different handwritten characters from 50 different alphabets.\n",
        "Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people.\n",
        "This dataset has been the baseline for any one-shot learning algorithm.\n",
        "\n",
        "\n",
        "Some of the machine learning algorithms used for learning this dataset over the years are listed below in order of accuracy:\n",
        "*  Hierarchical Bayesian Program Learning - 95.2%\n",
        "*  Convolutional Siamese Net                        - 92.0%\n",
        "*  Affine model                                                  - 81.8%\n",
        "*  Hierarchical Deep                                         - 65.2%\n",
        "*  Deep Boltzmann Machine                           - 62.0%\n",
        "*  Siamese Neural Net                                     - 58.3%\n",
        "*  Simple Stroke                                                - 35.2%\n",
        "*  1-Nearest Neighbor                                      - 21.7%\n",
        "\n",
        "\n",
        "This notebook implements a [Convolutional Siamese Neural Network](https://https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) using a background set of 30 alphabets for training and evaluate on set of 20 alphabets."
      ]
    },
    {
      "metadata": {
        "id": "7JehlDIHLpYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth, drive\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqpZVzsXLVEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "from PIL import Image, ImageFilter, ImageOps, ImageMath\n",
        "import numpy.random as rnd\n",
        "import pickle\n",
        "from time import sleep\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NKI63h3_VtZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# from tf.keras.models import Sequential  # This does not work!\n",
        "from tensorflow.python.keras.models import Model, Sequential\n",
        "from tensorflow.python.keras.layers import InputLayer, Input, Lambda\n",
        "from tensorflow.python.keras.layers import Reshape, MaxPooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "from tensorflow.python.keras.initializers import RandomNormal\n",
        "from tensorflow import test, logging\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.wrappers.scikit_learn import GridSearchCV\n",
        "logging.set_verbosity(tf.logging.ERROR)\n",
        "test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CDpwc_CmVtZU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVJ_ESVmxdDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "one_shot_path = os.path.join(\"drive\", \"My Drive\", \"Colab Notebooks\", \"One-Shot Classification\")\n",
        "background_path = os.path.join(one_shot_path, \"background\")\n",
        "evaluation_path = os.path.join(one_shot_path, \"evaluation\")\n",
        "recognition_model_path = os.path.join(one_shot_path, \"recognition_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xeeysjWca_w4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##creating training set\n",
        "train_data = np.ndarray(shape=(964, 20, 105, 105))\n",
        "train_alphabets = dict()\n",
        "\n",
        "#for alphabet in os.listdir(background_path):\n",
        "#  alphabet_path = os.path.join(background_path, alphabet)\n",
        "#  for character in os.listdir(alphabet_path):\n",
        "#    character_path = os.path.join(alphabet_path, character)\n",
        "#    for image in os.listdir(character_path):\n",
        "#      index = int(image[0:4]) - 1\n",
        "#      writer = int(image[5:7]) - 1\n",
        "#      train_data[index][writer] = np.array(Image.open(os.path.join(character_path, image)))\n",
        "#      train_alphabets[alphabet] = index if alphabet not in train_alphabets or train_alphabets[alphabet] > index else train_alphabets[alphabet]\n",
        "\n",
        "#with open(os.path.join(\"train.pickle\"), 'wb') as f:\n",
        "#  pickle.dump([train_data, train_alphabets], f, protocol=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LVoh_EoogUh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(os.path.join(one_shot_path, \"train.pickle\"), 'rb') as f:\n",
        "  train_data, train_alphabets = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDGhQ-1y3DzM",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title Inputs\n",
        "\n",
        "conv_activation = 'relu' #@param ['relu', 'softplus', 'tanh', 'sigmoid'] {type:\"string\"}\n",
        "dense_activation = 'sigmoid' #@param ['relu', 'softplus', 'tanh', 'sigmoid'] {type:\"string\"}\n",
        "learning_rate = 1e-2 #@param {type:\"number\"}\n",
        "conv_regularization_parameter = 1e-2 #@param {type:\"number\"}\n",
        "dense_regularization_parameter = 1e-4 #@param {type:\"number\"}\n",
        "batch_size = 128 #@param {type:\"slider\", min:0, max:1024, step:16}\n",
        "batches_per_epoch = 75 #@param {type:\"slider\", min:0, max:100, step:5}\n",
        "n_epochs = 200 #@param {type:\"slider\", min:25, max:500, step:25}\n",
        "\n",
        "\n",
        "batch_size = 1 if batch_size == 0 else batch_size\n",
        "batches_per_epoch = 1 if batches_per_epoch == 0 else batches_per_epoch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uUNq4VOt1VBS",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Data Augmentation\n",
        "image_size = 105 #@param {type:\"slider\", min:32, max:512, step:1}\n",
        "\n",
        "rotation_range = 10 #@param {type:\"slider\", min:0, max:90, step:1}\n",
        "width_shift_range = 2 #@param {type:\"slider\", min:0, max:10, step:0.1}\n",
        "height_shift_range = 2 #@param {type:\"slider\", min:0, max:10, step:0.1}\n",
        "shear_range = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "zoom_range = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MhqXSHZ3hG9u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this is the augmentation configuration we will use for training\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "def transform_image(image):\n",
        "  return datagen.apply_transform(image.reshape((image_size, image_size, 1)), \n",
        "                                 transform_parameters = \n",
        "                       {'theta': rnd.uniform(-rotation_range, rotation_range),\n",
        "                        'tx'   : rnd.uniform(-width_shift_range, width_shift_range),\n",
        "                        'ty'   : rnd.uniform(-height_shift_range, height_shift_range),\n",
        "                        'shear': rnd.uniform(-shear_range, shear_range),\n",
        "                        'zx'   : rnd.uniform(-zoom_range, zoom_range),\n",
        "                        'zy'   : rnd.uniform(-zoom_range, zoom_range)\n",
        "                       })\n",
        "\n",
        "#generate image pairs [x1, x2] with target y = 1/0 representing same/different\n",
        "def datagen_flow(datagen, val = False):\n",
        "    while True:\n",
        "      X1 = np.ndarray(shape=(batch_size, image_size, image_size, 1))\n",
        "      X2 = np.ndarray(shape=(batch_size, image_size, image_size, 1))\n",
        "      Y = np.ndarray(shape=(batch_size,))\n",
        "      \n",
        "      s_alphabets = sorted(train_alphabets.values())\n",
        "      a_indices = list(range(len(s_alphabets)))\n",
        "      times = batch_size//(2*len(a_indices))\n",
        "      remainder = (batch_size//2)%len(a_indices)\n",
        "      \n",
        "      aindices = a_indices*times + list(rnd.choice(a_indices, remainder))\n",
        "      rnd.shuffle(aindices)\n",
        "      \n",
        "      w_range = list(range(12, 20) if val else range(12))\n",
        "      \n",
        "      i = 0   \n",
        "      for a in aindices:\n",
        "        end_index = (len(train_data) if a+1 == len(s_alphabets) else s_alphabets[a+1])\n",
        "        c_range = list(range(s_alphabets[a], end_index))\n",
        "        \n",
        "        writers = rnd.choice(w_range, 2)\n",
        "        same = rnd.choice(c_range)\n",
        "        X1[2*i] = transform_image(train_data[same, writers[0]])\n",
        "        X2[2*i] = transform_image(train_data[same, writers[1]])\n",
        "        Y[2*i] = 1.0\n",
        "        \n",
        "        writers = rnd.choice(w_range, 2)\n",
        "        diff = rnd.choice(c_range, 2)\n",
        "        X1[2*i + 1] = transform_image(train_data[diff[0], writers[0]])\n",
        "        X2[2*i + 1] = transform_image(train_data[diff[1], writers[1]])\n",
        "        Y[2*i + 1] = 0.0\n",
        "        \n",
        "        i += 1\n",
        "        \n",
        "      yield [X1, X2], Y\n",
        "\n",
        "train_generator = datagen_flow(datagen)  \n",
        "\n",
        "# this is a similar generator, for validation data that takes only the remaining 8 writers\n",
        "train_dev_generator = datagen_flow(datagen, val=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qihLXdaHNSFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "w_init = RandomNormal(mean=0.0, stddev=1e-2)\n",
        "b_init = RandomNormal(mean=0.5, stddev=1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SzwK0wtPVtZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_shape=(image_size, image_size, 1)\n",
        "left_input = Input(input_shape)\n",
        "right_input = Input(input_shape)\n",
        "\n",
        "# Start construction of the Keras Sequential model.\n",
        "convnet = Sequential()\n",
        "\n",
        "# First convolutional layer with activation, batchnorm and max-pooling.\n",
        "convnet.add(Conv2D(kernel_size=10, strides=1, filters=64, padding='valid',\n",
        "                   input_shape=input_shape, bias_initializer=b_init,\n",
        "                   activation=conv_activation,\n",
        "                   name='layer_conv1', kernel_regularizer=l2(conv_regularization_parameter)))\n",
        "convnet.add(BatchNormalization(axis = 3, momentum=0.5, name = 'bn1'))\n",
        "convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling1\"))\n",
        "\n",
        "# Second convolutional layer with activation, batchnorm and max-pooling.\n",
        "convnet.add(Conv2D(kernel_size=7, strides=1, filters=128, padding='valid',\n",
        "                  kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                 activation=conv_activation, name='layer_conv2', kernel_regularizer=l2(conv_regularization_parameter)))\n",
        "convnet.add(BatchNormalization(axis = 3, name = 'bn2'))\n",
        "convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling2\"))\n",
        "\n",
        "# Third convolutional layer with activation, batchnorm and max-pooling.\n",
        "convnet.add(Conv2D(kernel_size=4, strides=1, filters=128, padding='valid',\n",
        "                  kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                 activation=conv_activation, name='layer_conv3', kernel_regularizer=l2(conv_regularization_parameter)))\n",
        "convnet.add(BatchNormalization(axis = 3, name = 'bn3'))\n",
        "convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling3\"))\n",
        "\n",
        "# Fourth convolutional layer with activation, batchnorm and max-pooling.\n",
        "convnet.add(Conv2D(kernel_size=4, strides=1, filters=256, padding='valid',\n",
        "                  kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                 activation=conv_activation, name='layer_conv4', kernel_regularizer=l2(conv_regularization_parameter)))\n",
        "convnet.add(BatchNormalization(axis = 3, name = 'bn4'))\n",
        "convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling4\"))\n",
        "\n",
        "# Flatten the 4-rank output of the convolutional layers\n",
        "# to 2-rank that can be input to a fully-connected / dense layer.\n",
        "convnet.add(Flatten())\n",
        "\n",
        "\n",
        "# First fully-connected / dense layer with activation.\n",
        "convnet.add(Dense(4096, activation=dense_activation,\n",
        "                 kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                 name = \"dense_1\", kernel_regularizer=l2(dense_regularization_parameter)))\n",
        "convnet.add(BatchNormalization(axis = 1, name = 'bn5'))\n",
        "\n",
        "#call the convnet Sequential model on each of the input tensors so params will be shared\n",
        "encoded_l = convnet(left_input)\n",
        "encoded_r = convnet(right_input)\n",
        "\n",
        "#layer to merge two encoded inputs with the l1 distance between them\n",
        "L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "\n",
        "#call this layer on list of two input tensors.\n",
        "L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "\n",
        "prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
        "\n",
        "model = Model(inputs=[left_input,right_input],outputs=prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VnG3DzENVtZw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.optimizers import SGD, Adam\n",
        "\n",
        "#optimizer = SGD(lr=learning_rate, momentum=0.5)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "steps_train = batches_per_epoch\n",
        "steps_validation = batches_per_epoch "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-kEFic269Ll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler, ReduceLROnPlateau\n",
        "\n",
        "model_checkpoint = ModelCheckpoint(recognition_model_path, monitor='val_loss',\n",
        "                                   save_best_only=True, period=10)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lambda epoch, lr: 0.99*lr)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
        "                              patience=5, min_lr=1e-4)\n",
        "\n",
        "class LearningRateFinder(Callback):\n",
        "  def __init__(self, steps=100, period=10):\n",
        "    super(LearningRateFinder, self).__init__()\n",
        "    self.steps = steps\n",
        "    self.batch_size=batch_size\n",
        "    self.period = period\n",
        "    self.best_lr = 1e-4\n",
        "    self.best_loss = 1000\n",
        "    self.find_lr = True\n",
        "    self.current_lr = None\n",
        "    self.training_path = os.path.join(one_shot_path, \"training_model.h5\")\n",
        "    self.model_weights = None\n",
        "  \n",
        "  def reset_values(self):\n",
        "    K.set_value(self.model.optimizer.lr, self.best_lr)\n",
        "    self.best_lr = 1e-4\n",
        "    self.best_loss = 1000\n",
        "    self.model = load_model(self.training_path)\n",
        "    \n",
        "  def on_train_begin(self, logs={}):\n",
        "    return\n",
        "\n",
        "  def on_train_end(self, logs={}):\n",
        "    return\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs={}):\n",
        "    self.find_lr = epoch % self.period == 0\n",
        "    if epoch % self.period == 1:\n",
        "      print(\"Learning Rate: \" + \"{0:.2g}\".format(K.get_value(self.model.optimizer.lr)))\n",
        "    if(self.find_lr):\n",
        "      self.current_lr = K.get_value(self.model.optimizer.lr)\n",
        "      self.model.save(self.training_path)\n",
        "      self.model_weights = self.model.get_weights()\n",
        "      \n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(self.find_lr):\n",
        "      self.reset_values()\n",
        "    return \n",
        "\n",
        "  def on_batch_begin(self, batch, logs={}):\n",
        "    if(self.find_lr):\n",
        "      K.set_value(self.model.optimizer.lr, 10**(2*batch/self.steps + np.log10(self.current_lr) - 1))\n",
        "    return\n",
        "\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    if(self.find_lr):\n",
        "      loss = logs.get('loss')\n",
        "      if loss < self.best_loss:\n",
        "        self.best_loss = loss\n",
        "        self.best_lr = K.get_value(self.model.optimizer.lr)\n",
        "      elif loss >= 1.25*self.best_loss:\n",
        "        self.find_lr = False\n",
        "        self.reset_values()\n",
        "      self.model.set_weights(self.model_weights)\n",
        "      \n",
        "    return\n",
        "  \n",
        "lr_finder = LearningRateFinder(steps=steps_train, period=n_epochs//4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOrQ3XKbVtZ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator, \n",
        "                    steps_per_epoch = steps_train,\n",
        "                    epochs=n_epochs,\n",
        "                    validation_data = train_dev_generator,\n",
        "                    validation_steps = steps_validation,\n",
        "                    callbacks = [model_checkpoint, lr_scheduler, reduce_lr]\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P8lGyA-0xvkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = load_model(recognition_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4FkInu-bquq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##creating test set\n",
        "test_data = np.ndarray(shape=(659, 20, 105, 105))\n",
        "test_alphabets = dict()\n",
        "\n",
        "#for alphabet in os.listdir(evaluation_path):\n",
        "#  alphabet_path = os.path.join(evaluation_path, alphabet)\n",
        "#  for character in os.listdir(alphabet_path):\n",
        "#    character_path = os.path.join(alphabet_path, character)\n",
        "#    for image in os.listdir(character_path):\n",
        "#      index = int(image[0:4]) - 965\n",
        "#      writer = int(image[5:7]) - 1\n",
        "#      test_data[index][writer] = np.array(Image.open(os.path.join(character_path, image)))\n",
        "#      test_alphabets[alphabet] = index if alphabet not in test_alphabets or test_alphabets[alphabet] > index else test_alphabets[alphabet]\n",
        "\n",
        "#with open(os.path.join(\"test.pickle\"), 'wb') as f:\n",
        "#  pickle.dump([test_data, test_alphabets], f, protocol=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sGGBO9Wfa4DO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(os.path.join(one_shot_path, \"test.pickle\"), 'rb') as f:\n",
        "  test_data, test_alphabets = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xBmaH5FfpQXK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = 20\n",
        "st_alphabets = sorted(test_alphabets.values())\n",
        "correct = 0\n",
        "show = True\n",
        "for i in range(len(st_alphabets)):\n",
        "  end_index = len(test_data) if i+1 == len(st_alphabets) else st_alphabets[i+1] \n",
        "  c_range = list(range(st_alphabets[i],end_index))\n",
        "  \n",
        "  for j in range(2):\n",
        "    c_list = rnd.choice(c_range, N)\n",
        "    w_list = rnd.choice(range(20), 2)\n",
        "    \n",
        "    for c_i in range(N):\n",
        "      image = test_data[c_list[c_i]][w_list[0]]\n",
        "      \n",
        "      X1 = np.array([image]*N).reshape((N, image_size, image_size, 1))\n",
        "      X2 = np.array(test_data[c_list][w_list[1]]).reshape((N, image_size, image_size, 1))\n",
        "      if show and c_i == 2 and i == 3:\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "        for m in range(N):\n",
        "          plt.imshow(test_data[c_list[m]][w_list[1]])\n",
        "          plt.show()\n",
        "        \n",
        "        \n",
        "      targets = np.zeros((N,))\n",
        "      targets[c_i] = 1\n",
        "      predictions = model.predict([X1, X2])\n",
        "      \n",
        "      if show and c_i == 2 and i == 3:\n",
        "        print(targets)\n",
        "        print(predictions)\n",
        "        show = False\n",
        "        \n",
        "      if(np.argmax(predictions) == np.argmax(targets)):\n",
        "        correct += 1\n",
        "\n",
        "print(str(N) + \"-Way Classification Accuracy: \" + \"{0:.2f}\".format(correct/(N*20*2)))  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}