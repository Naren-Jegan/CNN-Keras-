{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "One-Shot Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Naren-Jegan/Deep-Learning-Keras/blob/master/One_Shot_Classification_V2.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "B5KR1KHjuk6N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One Shot Learning on Omniglot Dataset\n",
        "\n",
        "The [Omniglot](https://github.com/brendenlake/omniglot) dataset contains 1623 different handwritten characters from 50 different alphabets.\n",
        "Each of the 1623 characters was drawn online via Amazon's Mechanical Turk by 20 different people.\n",
        "This dataset has been the baseline for any one-shot learning algorithm.\n",
        "\n",
        "\n",
        "Some of the machine learning algorithms used for learning this dataset over the years are listed below in order of accuracy:\n",
        "*  Hierarchical Bayesian Program Learning - 95.2%\n",
        "*  Convolutional Siamese Net                        - 92.0%\n",
        "*  Affine model                                                  - 81.8%\n",
        "*  Hierarchical Deep                                         - 65.2%\n",
        "*  Deep Boltzmann Machine                           - 62.0%\n",
        "*  Siamese Neural Net                                     - 58.3%\n",
        "*  Simple Stroke                                                - 35.2%\n",
        "*  1-Nearest Neighbor                                      - 21.7%\n",
        "\n",
        "\n",
        "This notebook implements a [Convolutional Siamese Neural Network](https://https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) using a background set of 30 alphabets for training and evaluate on set of 20 alphabets."
      ]
    },
    {
      "metadata": {
        "id": "099uR_fO1rqt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# How is the data?\n",
        "\n",
        "The Omniglot data set contains 50 alphabets total. It is split into a background set of 30 alphabets and an evaluation set of 20 alphabets.\n",
        "\n",
        "To compare with the results in the paper [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf), only the background set should be used to learn general knowledge about characters (e.g., hyperparameter inference or feature learning). One-shot learning results are reported using alphabets from the evaluation set.\n",
        "\n",
        "# Where is the data stored?\n",
        "\n",
        "The actual zipped Omniglot dataset, extracted and processed data (Pickled data) are stored in my google drive folder named \"One-Shot Classification\". To mount the drive, we use 2 modules, namely:\n",
        "\n",
        "\n",
        "1.   google.colab.auth - for authentication to drive\n",
        "2.   google.colab.drive - for mounting from drive\n",
        "\n",
        "# Hyper Parameter Optimisation - HyperOpt\n",
        "\n",
        "\n",
        "Hyperopt is a Python library for serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.\n",
        "\n",
        "[Github Link](http://github.com/hyperopt/hyperopt)\n",
        "\n",
        "In thiis notebook we use hyperopt trials to find the best hyper-parameters for the One-Shot Classification model. Hyperopt uses Bayesian optimization techniques to search. Tree-structured Parzen Estimator - TPE is used in this implementation.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7JehlDIHLpYa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "ed9db227-6d81-4b7f-e17a-4ea6014eb0b0"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q hyperopt\n",
        "from google.colab import auth, drive\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B4ZNT7RO6C91",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imported Normal Libraries\n",
        "\n",
        "\n",
        "*   matplotlib.pyplot - To plot images\n",
        "*   numpy - Tensor manipulation\n",
        "*   os - File system manipulation after mounting from Drive\n",
        "*   PIL.Image - To convert image files into numpy arrays\n",
        "*   pickle - To store objects are files (mainly numpy arrays in this implementation)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DqpZVzsXLVEY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy.random as rnd\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dkg4XY-g7hn3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tensorflow, Keras and Hyperopt Imports\n",
        "*   **Keras**\n",
        "   *  models - To create, load and save models\n",
        "   *  layers - To create different types of layers\n",
        "   *  preprocessing.image - To generate image transformations\n",
        "   *  backend - Utilities to modelfy Keras objects\n",
        "   *  optimizers - To create optimizers\n",
        "   *  regularizers - To create regularizers\n",
        "   *  initializers - To create kernel and bias initializers\n",
        "   *  legacy - To refer keras source\n",
        "*  **Tensorflow**\n",
        "   *  logging - To avoid unnecessary prints\n",
        "   *  test - To check if GPU is available\n",
        "*  **Hyperopt**\n",
        "   *  hp - All randomization functions\n",
        "   *  fmin - Optimization function to minimize objective\n",
        "   *  tpe - Tree-structured Parzen Estimator\n",
        "   *  Trails - Object to hold each run information\n",
        "   *  STATUS_OK - Flag to proceed to next run"
      ]
    },
    {
      "metadata": {
        "id": "NKI63h3_VtZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cdd7b841-4de0-4007-8b0a-47fd5edab7c5"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Model, Sequential\n",
        "from tensorflow.python.keras.layers import InputLayer, Input, Lambda\n",
        "from tensorflow.python.keras.layers import Reshape, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.python.keras.layers import Conv2D, Dense, Flatten\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.models import load_model\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras.optimizers import Optimizer\n",
        "from tensorflow.python.keras.regularizers import l2\n",
        "from tensorflow.python.keras.initializers import RandomNormal\n",
        "from tensorflow import test, logging\n",
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "from keras.legacy import interfaces\n",
        "\n",
        "logging.set_verbosity(tf.logging.ERROR)\n",
        "test.gpu_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "CDpwc_CmVtZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bc56c8d-5640-4a32-f127-d35c70f20b11"
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0-rc1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "M3n00DxT_nZ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# File System Structure\n",
        "\n",
        "One-Shot Classification\n",
        "\n",
        "\n",
        "* Background\n",
        "  * Alphabet_of_the_Magi\n",
        "    *  character01\n",
        "      *  0709_01.png\n",
        "      *  0709_02.png\n",
        "      *  .\n",
        "      *  .\n",
        "      *  .\n",
        "    *  character02\n",
        "    *  .\n",
        "    *  .\n",
        "    *  .\n",
        "  * Anglo-Saxon_Futhorc\n",
        "  * .\n",
        "  * .\n",
        "  * .\n",
        "\n",
        "* Evaluation\n",
        "  * Angelic\n",
        "    *  character01\n",
        "      *  0965_01.png\n",
        "      *  0965_02.png\n",
        "      *  .\n",
        "      *  .\n",
        "      *  .\n",
        "    *  character02\n",
        "    *  .\n",
        "    *  .\n",
        "    *  .\n",
        "  * Atemayar_Qelisayer\n",
        "  * .\n",
        "  * .\n",
        "  * .\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EVJ_ESVmxdDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#path to main folder\n",
        "one_shot_path = os.path.join(\"drive\", \"My Drive\", \"Colab Notebooks\", \"One-Shot Classification\")\n",
        "\n",
        "#path to background and evaluation data\n",
        "background_path = os.path.join(one_shot_path, \"background\")\n",
        "evaluation_path = os.path.join(one_shot_path, \"evaluation\")\n",
        "\n",
        "#path to final model\n",
        "recognition_model_path = os.path.join(one_shot_path, \"recognition_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MaMJpKcSJs33",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and Pickling\n",
        "\n",
        "Both the training and test data are converted in a 4 dimensional array of the following form,\n",
        "\n",
        ">(Character_id, Writer_id, Pixel_X, Pixel_Y)\n",
        "\n",
        "> where,\n",
        "\n",
        "\n",
        "*   Character_id - number given in the filename as **number**_writer.png\n",
        "*   Writer_id - writer given in the filename as number_**writer**.png\n",
        "*   Pixel_X - X coordinate of pixel value\n",
        "*   Pixel_Y - Y coordinate of pixel value\n",
        "\n",
        "Along with this data, store a mapping contating the alphabets and it's starting character_id number\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xeeysjWca_w4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##creating training set\n",
        "train_data = np.ndarray(shape=(964, 20, 105, 105))\n",
        "train_alphabets = dict()\n",
        "\n",
        "#Preprocessing\n",
        "#for alphabet in os.listdir(background_path):\n",
        "#  alphabet_path = os.path.join(background_path, alphabet)\n",
        "#  for character in os.listdir(alphabet_path):\n",
        "#    character_path = os.path.join(alphabet_path, character)\n",
        "#    for image in os.listdir(character_path):\n",
        "#      index = int(image[0:4]) - 1\n",
        "#      writer = int(image[5:7]) - 1\n",
        "#      train_data[index][writer] = np.array(Image.open(os.path.join(character_path, image)))\n",
        "#      train_alphabets[alphabet] = index if alphabet not in train_alphabets or train_alphabets[alphabet] > index else train_alphabets[alphabet]\n",
        "\n",
        "#with open(os.path.join(\"train.pickle\"), 'wb') as f:\n",
        "#  pickle.dump([train_data, train_alphabets], f, protocol=2)\n",
        "\n",
        "##creating test set\n",
        "test_data = np.ndarray(shape=(659, 20, 105, 105))\n",
        "test_alphabets = dict()\n",
        "\n",
        "#Preprocessing\n",
        "#for alphabet in os.listdir(evaluation_path):\n",
        "#  alphabet_path = os.path.join(evaluation_path, alphabet)\n",
        "#  for character in os.listdir(alphabet_path):\n",
        "#    character_path = os.path.join(alphabet_path, character)\n",
        "#    for image in os.listdir(character_path):\n",
        "#      index = int(image[0:4]) - 965\n",
        "#      writer = int(image[5:7]) - 1\n",
        "#      test_data[index][writer] = np.array(Image.open(os.path.join(character_path, image)))\n",
        "#      test_alphabets[alphabet] = index if alphabet not in test_alphabets or test_alphabets[alphabet] > index else test_alphabets[alphabet]\n",
        "\n",
        "#with open(os.path.join(\"test.pickle\"), 'wb') as f:\n",
        "#  pickle.dump([test_data, test_alphabets], f, protocol=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JOZwcwdoLTXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading Preprocessed training and test data (train.pickle and test.pickle)"
      ]
    },
    {
      "metadata": {
        "id": "4LVoh_EoogUh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(os.path.join(one_shot_path, \"train.pickle\"), 'rb') as f:\n",
        "  train_data, train_alphabets = pickle.load(f, encoding='latin1')\n",
        "  \n",
        "with open(os.path.join(one_shot_path, \"test.pickle\"), 'rb') as f:\n",
        "  test_data, test_alphabets = pickle.load(f, encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDGhQ-1y3DzM",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "image_size = 105"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YDadpHK4LmN4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Augmentation Ranges\n",
        "During data generation, images are transformed to provide a more robust training. The parameters for the transformations are defined as follows:\n",
        "\n",
        "*   Rotation Range - maximum degrees upto which image can be rotated clockwise and anti-clockwise\n",
        "*   Width Shift Range - maximum number of pixels upto which image can be shifted to the left or right\n",
        "*   Height Shift Range - maximum number of pixels upto which image can be shifted to the up or down\n",
        "*   Shear Range - maximum degree of shearing allowed\n",
        "*   Zoom Range\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uUNq4VOt1VBS",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title Data Augmentation\n",
        "rotation_range = 10 #@param {type:\"slider\", min:0, max:90, step:1}\n",
        "width_shift_range = 2 #@param {type:\"slider\", min:0, max:10, step:0.1}\n",
        "height_shift_range = 2 #@param {type:\"slider\", min:0, max:10, step:0.1}\n",
        "shear_range = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "zoom_range = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y8Qg2MGUOa5v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch Generation\n",
        "\n",
        "\n",
        "*   Create X1, X2 which will contain batch_size number of images paired against one other for comparison \n",
        "*   Create Y which will contain the results of comparison for the whole batch\n",
        "*   Each alphabet should get equal representation in the training\n",
        "  *  s_alphabets -> alphabet's starting character_ids in sorted order\n",
        "  *  times -> number of times each alphabet can be represented in a single batch equally for both same and different pairs\n",
        "  *  reminder -> number of times alphabets have to be picked at random cause batch size is not a multiple of the number of alphabets\n",
        "*  For each alphabet chosen, create same (writer) and different (character) pairs\n",
        "  *  w_range -> writers to chose from\n",
        "  *  c_range -> characters to choose from\n",
        "  *  transform_image -> image augmentation function\n",
        "*  Yield created batch\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MhqXSHZ3hG9u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this is the augmentation configuration we will use for training\n",
        "datagen = ImageDataGenerator()\n",
        "\n",
        "def transform_image(image):\n",
        "  return datagen.apply_transform(image.reshape((image_size, image_size, 1)), \n",
        "                                 transform_parameters = \n",
        "                       {'theta': rnd.uniform(-rotation_range, rotation_range),\n",
        "                        'tx'   : rnd.uniform(-width_shift_range, width_shift_range),\n",
        "                        'ty'   : rnd.uniform(-height_shift_range, height_shift_range),\n",
        "                        'shear': rnd.uniform(-shear_range, shear_range),\n",
        "                        'zx'   : rnd.uniform(-zoom_range, zoom_range),\n",
        "                        'zy'   : rnd.uniform(-zoom_range, zoom_range)\n",
        "                       })\n",
        "\n",
        "#generate image pairs [x1, x2] with target y = 1/0 representing same/different\n",
        "def datagen_flow(datagen):\n",
        "    while True:\n",
        "      X1 = np.ndarray(shape=(batch_size, image_size, image_size, 1))\n",
        "      X2 = np.ndarray(shape=(batch_size, image_size, image_size, 1))\n",
        "      Y = np.ndarray(shape=(batch_size,))\n",
        "      \n",
        "      s_alphabets = sorted(train_alphabets.values())\n",
        "      a_indices = list(range(len(s_alphabets)))\n",
        "      times = batch_size//(2*len(a_indices))\n",
        "      remainder = (batch_size//2)%len(a_indices)\n",
        "      \n",
        "      aindices = a_indices*times + list(rnd.choice(a_indices, remainder))\n",
        "      rnd.shuffle(aindices)\n",
        "      \n",
        "      w_range = list(range(20))\n",
        "      \n",
        "      i = 0   \n",
        "      for a in aindices:\n",
        "        end_index = (len(train_data) if a+1 == len(s_alphabets) else s_alphabets[a+1])\n",
        "        c_range = list(range(s_alphabets[a], end_index))\n",
        "        \n",
        "        writers = rnd.choice(w_range, 2)\n",
        "        same = rnd.choice(c_range)\n",
        "        X1[2*i] = transform_image(train_data[same, writers[0]])\n",
        "        X2[2*i] = transform_image(train_data[same, writers[1]])\n",
        "        Y[2*i] = 1.0\n",
        "        \n",
        "        writers = rnd.choice(w_range, 2)\n",
        "        diff = rnd.choice(c_range, 2)\n",
        "        X1[2*i + 1] = transform_image(train_data[diff[0], writers[0]])\n",
        "        X2[2*i + 1] = transform_image(train_data[diff[1], writers[1]])\n",
        "        Y[2*i + 1] = 0.0\n",
        "        \n",
        "        i += 1\n",
        "        \n",
        "      yield [X1, X2], Y\n",
        "\n",
        "train_generator = datagen_flow(datagen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wKN30y58YbYW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Modification of Keras SGD Optimizer\n",
        "A few modifications have been done to the original Keras SGD optimizer to include\n",
        "\n",
        "*   Learning rate for each layer\n",
        "*   Maximum momentum for each layer\n",
        "*   Linearly increase momentum from 0.5 to Maximum momemtum based on the number of epochs\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PqLa_1Ct1OsS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Modified_SGD(Optimizer):\n",
        "    \"\"\" Modified Stochastic gradient descent optimizer.\n",
        "    Reorganized SGD to allow layer-wise momentum and learning-rate\n",
        "    Includes support for momentum,\n",
        "    learning rate decay, and Nesterov momentum.\n",
        "    Includes the possibility to add multipliers to different\n",
        "    learning rates in each layer.\n",
        "    # Arguments\n",
        "        lr: float >= 0. Learning rate.\n",
        "        momentum: float >= 0. Parameter updates momentum.\n",
        "        decay: float >= 0. Learning rate decay over each update.\n",
        "        nesterov: boolean. Whether to apply Nesterov momentum.\n",
        "        lr_values: dictionary with learning rate for a specific layer\n",
        "        for example:\n",
        "            # Setting the Learning rate multipliers\n",
        "            lr_values = {}\n",
        "            lr_values['conv1']=1\n",
        "        momentum_values: dictionary with momentum for a specific layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lr=1, momentum=0.5, decay=0., n_epochs=200,\n",
        "                 nesterov=False, lr_values=None, momentum_values=None, **kwargs):\n",
        "        super(Modified_SGD, self).__init__(**kwargs)\n",
        "        with K.name_scope(self.__class__.__name__):\n",
        "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
        "            self.lr = K.variable(lr, name='lr')\n",
        "            self.momentum = K.variable(momentum, name='momentum')\n",
        "            self.decay = K.variable(decay, name='decay')\n",
        "        self.initial_decay = decay\n",
        "        self.nesterov = nesterov\n",
        "        self.lr_values = lr_values\n",
        "        self.momentum_values = momentum_values\n",
        "        self.n_epochs = n_epochs\n",
        "\n",
        "    @interfaces.legacy_get_updates_support\n",
        "    def get_updates(self, loss, params):\n",
        "        grads = self.get_gradients(loss, params)\n",
        "        self.updates = [K.update_add(self.iterations, 1)]\n",
        "\n",
        "        lr = self.lr\n",
        "        if self.initial_decay > 0:\n",
        "            lr *= (1. / (1. + self.decay * K.cast(self.iterations,\n",
        "                                                  K.dtype(self.decay))))\n",
        "        \n",
        "        \n",
        "        # momentum\n",
        "        shapes = [K.int_shape(p) for p in params]\n",
        "        moments = [K.zeros(shape) for shape in shapes]\n",
        "        self.weights = [self.iterations] + moments\n",
        "        for p, g, m in zip(params, grads, moments):\n",
        "\n",
        "            if self.lr_values != None:\n",
        "                if p.name in self.lr_values:\n",
        "                    new_lr = lr * self.lr_values[p.name]\n",
        "                else:\n",
        "                    new_lr = lr\n",
        "            else:\n",
        "                new_lr = lr\n",
        "\n",
        "            if self.momentum_values != None:\n",
        "                if p.name in self.momentum_values:\n",
        "                    new_momentum = ((self.momentum_values[p.name] - self.momentum)/self.n_epochs)*self.iterations + self.momentum  \n",
        "                else:\n",
        "                    new_momentum = self.momentum\n",
        "            else:\n",
        "                new_momentum = self.momentum\n",
        "            \n",
        "            # velocity\n",
        "            v = new_momentum * m - new_lr * g  \n",
        "            self.updates.append(K.update(m, v))\n",
        "\n",
        "            if self.nesterov:\n",
        "                new_p = p + new_momentum * v - new_lr * g\n",
        "            else:\n",
        "                new_p = p + v\n",
        "\n",
        "            # Apply constraints.\n",
        "            if getattr(p, 'constraint', None) is not None:\n",
        "                new_p = p.constraint(new_p)\n",
        "\n",
        "            self.updates.append(K.update(p, new_p))\n",
        "        return self.updates\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'lr': float(K.get_value(self.lr)),\n",
        "                  'momentum': float(K.get_value(self.momentum)),\n",
        "                  'decay': float(K.get_value(self.decay)),\n",
        "                  'nesterov': self.nesterov,\n",
        "                  'lr_values': self.lr_values,\n",
        "                  'momentum_values': self.momentum_values}\n",
        "        base_config = super(Modified_SGD, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYIUMU1sZXiR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Weight and Bias Initilization\n",
        "*  Weights are randomly initialized with a mean 0 and standard deviation 0.01 over a normal distribution\n",
        "*  Biases are randomly initialized with a mean 0.5 and standard deviation 0.01 over a normal distribution"
      ]
    },
    {
      "metadata": {
        "id": "qihLXdaHNSFj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w_init = RandomNormal(mean=0.0, stddev=1e-2)\n",
        "b_init = RandomNormal(mean=0.5, stddev=1e-2)\n",
        "\n",
        "#image shape\n",
        "input_shape=(image_size, image_size, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3npFsg2wZ_oe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 20-Way One-Shot Classification Task\n",
        "![Example Image](data:image/png;base64,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)\n",
        "\n",
        "For validating and testing, twice for every alphabet (10 for each validation and test),\n",
        "*   Randomly select 20 characters\n",
        "*   Randomly select 2 writers\n",
        "*   For each character chosen:\n",
        "  *  X1 -> 20 copies of the character written by the first writer\n",
        "  *  X2 -> all 20 characters by the second writer\n",
        "  *  check if the prediction for [X1, X2] as input activates the correct character\n",
        "  *  If correct count it\n",
        "*  Finally accuracy -> count/2\\*20\\*no_of_alphabets -> count/400"
      ]
    },
    {
      "metadata": {
        "id": "vHlxoQHV6oVD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def validate(model, test = False, show = False):\n",
        "  N = 20\n",
        "  \n",
        "  st_alphabets = sorted(test_alphabets.values())\n",
        "  max_index = len(test_data) if test else st_alphabets[10]\n",
        "  st_alphabets = st_alphabets[10:20] if test else st_alphabets[0:10]\n",
        "  correct = 0\n",
        "  for i in range(len(st_alphabets)):\n",
        "    end_index = max_index if i+1 == len(st_alphabets) else st_alphabets[i+1] \n",
        "    c_range = list(range(st_alphabets[i],end_index))\n",
        "\n",
        "    for j in range(2):\n",
        "      c_list = rnd.choice(c_range, N)\n",
        "      w_list = rnd.choice(range(20), 2)\n",
        "\n",
        "      for c_i in range(N):\n",
        "        image = test_data[c_list[c_i]][w_list[0]]\n",
        "\n",
        "        X1 = np.array([image]*N).reshape((N, image_size, image_size, 1))\n",
        "        X2 = np.array(test_data[c_list][w_list[1]]).reshape((N, image_size, image_size, 1))\n",
        "        if show and c_i == 2 and i == 3:\n",
        "          plt.imshow(image)\n",
        "          plt.show()\n",
        "          for m in range(N):\n",
        "            plt.imshow(test_data[c_list[m]][w_list[1]])\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "        targets = np.zeros((N,))\n",
        "        targets[c_i] = 1\n",
        "        predictions = model.predict([X1, X2])\n",
        "\n",
        "        if show and c_i == 2 and i == 3:\n",
        "          print(targets)\n",
        "          print(predictions)\n",
        "          show = False\n",
        "          \n",
        "        if(np.argmax(predictions) == np.argmax(targets)):\n",
        "          correct += 1\n",
        "  return (100*correct/(N*10*2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UtLnA0H1cvsg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reduce learning rate by 1% after each epoch"
      ]
    },
    {
      "metadata": {
        "id": "r-kEFic269Ll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#CallBacks\n",
        "from tensorflow.python.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lambda epoch, lr: 0.99*lr)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aj0t0V4Zc66H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# One-Shot Model building, compiling, fitting and validating\n",
        "\n",
        "**Model Architecture**\n",
        "\n",
        "![One Shot Model Architecture](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQThMusu8b2uK8kGwrFsg-cuZXaN8Wc7HkfgyiM-8YAfCfN_2uiJQ)\n",
        "\n",
        "**Model Keras Design**\n",
        "\n",
        "*  The model in the image is created by creating a Sequential keras model until the fully connected layer\n",
        "  *  Each layer has it's own regularization parameter and a batch normalizer\n",
        "*  A Lambda layer is introduced to perform a L1 distance calculation between the 4096 embedding outputs of the twin networks\n",
        "* The L1 distance is then given to a sigmoid to classify as same/different (1/0)\n",
        "*  Finally this is wrapped up in a functional model that takes 2 input images for the twin networks and outputs the sigmoid value of the L1 distance\n",
        "\n",
        "**Model Compilation:**\n",
        "The above mentioned model is compiled with the modified SGD optimizer, binary_crossentropy loss and accuracy as the metric\n",
        "\n",
        "**Model Fitting:**\n",
        "The model is then trained for the given n_epochs, each epoch training on steps_per_epoch number of  batchs with the given starting learning rates for each layer and final momentums for each layer\n",
        "\n",
        "**Model Validation:**\n",
        "The model trained thus is validated using the validate function defined earlier and the negative of validation accuracy is returned for the hyper-parameter optimization procedure to minimize\n"
      ]
    },
    {
      "metadata": {
        "id": "SzwK0wtPVtZt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def One_Shot_Model(params):\n",
        "  \n",
        "  print(params)\n",
        "  \n",
        "  lr_values = dict()\n",
        "  lr_values['layer_conv1'] = params['l_c1']\n",
        "  lr_values['layer_conv2'] = params['l_c2']\n",
        "  lr_values['layer_conv3'] = params['l_c3']\n",
        "  lr_values['layer_conv4'] = params['l_c4']\n",
        "  lr_values['layer_dense1'] = params['l_d1']\n",
        "  \n",
        "  momentum_values = dict()\n",
        "  momentum_values['layer_conv1'] = params['m_c1']\n",
        "  momentum_values['layer_conv2'] = params['m_c2']\n",
        "  momentum_values['layer_conv3'] = params['m_c3']\n",
        "  momentum_values['layer_conv4'] = params['m_c4']\n",
        "  momentum_values['layer_dense1'] = params['m_d1']\n",
        "  \n",
        "  reg_values = dict()\n",
        "  reg_values['layer_conv1'] = params['r_c1']\n",
        "  reg_values['layer_conv2'] = params['r_c2']\n",
        "  reg_values['layer_conv3'] = params['r_c3']\n",
        "  reg_values['layer_conv4'] = params['r_c4']\n",
        "  reg_values['layer_dense1'] = params['r_d1']\n",
        "  \n",
        "  left_input = Input(input_shape)\n",
        "  right_input = Input(input_shape)\n",
        "\n",
        "  # Start construction of the Keras Sequential model.\n",
        "  convnet = Sequential()\n",
        "\n",
        "  # First convolutional layer with activation, batchnorm and max-pooling.\n",
        "  convnet.add(Conv2D(kernel_size=10, strides=1, filters=64, padding='valid',\n",
        "                     input_shape=input_shape, bias_initializer=b_init,\n",
        "                     activation='relu',\n",
        "                     name='layer_conv1', kernel_regularizer=l2(reg_values['layer_conv1'])))\n",
        "  convnet.add(BatchNormalization(axis = 3, name = 'bn1'))\n",
        "  convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling1\"))\n",
        "\n",
        "  # Second convolutional layer with activation, batchnorm and max-pooling.\n",
        "  convnet.add(Conv2D(kernel_size=7, strides=1, filters=128, padding='valid',\n",
        "                    kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                    activation='relu', \n",
        "                    name='layer_conv2', kernel_regularizer=l2(reg_values['layer_conv2'])))\n",
        "  convnet.add(BatchNormalization(axis = 3, name = 'bn2'))\n",
        "  convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling2\"))\n",
        "\n",
        "  # Third convolutional layer with activation, batchnorm and max-pooling.\n",
        "  convnet.add(Conv2D(kernel_size=4, strides=1, filters=128, padding='valid',\n",
        "                    kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                    activation='relu', \n",
        "                    name='layer_conv3', kernel_regularizer=l2(reg_values['layer_conv3'])))\n",
        "  convnet.add(BatchNormalization(axis = 3, name = 'bn3'))\n",
        "  convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling3\"))\n",
        "\n",
        "  # Fourth convolutional layer with activation, batchnorm and max-pooling.\n",
        "  convnet.add(Conv2D(kernel_size=4, strides=1, filters=256, padding='valid',\n",
        "                    kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                    activation='relu', \n",
        "                    name='layer_conv4', kernel_regularizer=l2(reg_values['layer_conv4'])))\n",
        "  convnet.add(BatchNormalization(axis = 3, name = 'bn4'))\n",
        "  convnet.add(MaxPooling2D(pool_size=2, strides=2, name=\"max_pooling4\"))\n",
        "\n",
        "  # Flatten the 4-rank output of the convolutional layers\n",
        "  # to 2-rank that can be input to a fully-connected / dense layer.\n",
        "  convnet.add(Flatten())\n",
        "\n",
        "\n",
        "  # First fully-connected / dense layer with activation.\n",
        "  convnet.add(Dense(4096, activation='sigmoid',\n",
        "                   kernel_initializer=w_init, bias_initializer=b_init,\n",
        "                   name = \"layer_dense1\", kernel_regularizer=l2(reg_values['layer_dense1'])))\n",
        "  convnet.add(BatchNormalization(axis = 1, name = 'bn5'))\n",
        "\n",
        "  #call the convnet Sequential model on each of the input tensors so params will be shared\n",
        "  encoded_l = convnet(left_input)\n",
        "  encoded_r = convnet(right_input)\n",
        "\n",
        "  #layer to merge two encoded inputs with the l1 distance between them\n",
        "  L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
        "\n",
        "  #call this layer on list of two input tensors.\n",
        "  L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "\n",
        "  prediction = Dense(1,activation='sigmoid',bias_initializer=b_init)(L1_distance)\n",
        "  \n",
        "  model = Model(inputs=[left_input,right_input],outputs=prediction)\n",
        "  \n",
        "  optimizer = Modified_SGD(lr=1, lr_values=lr_values, momentum_values=momentum_values, momentum=0.5, n_epochs=params['epochs'])\n",
        "  \n",
        "  model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=params['steps_per_epoch'],\n",
        "      epochs=params['epochs'],\n",
        "      callbacks=[lr_scheduler],\n",
        "      verbose=False\n",
        "  )\n",
        "  val_acc = validate(model)\n",
        "  print('Validation accuracy:', val_acc)\n",
        "  return {'loss': -val_acc, 'status': STATUS_OK, 'model': model}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bn6INMNmv0zk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trials_path = os.path.join(one_shot_path, 'trials.hyperopt')\n",
        "trials=Trials()\n",
        "\n",
        "#with open(trials_path, 'wb') as f:\n",
        "#  pickle.dump(trials, f, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2bQgASmi048",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hyper-Parameter Space\n",
        "*  Learning rates for each layer - [0.0001, 0.1]\n",
        "*  Final momentum for each layer - [0.5, 1]\n",
        "*  Regularization Parameter - [0, 0.1]\n",
        "*  Epochs - (100, 150, 200)\n",
        "*  Steps per epochs/ batches per epoch - (25, 50, 75, 100)\n",
        "\n",
        "In the original paper, each epoch trained a maximum of 150,000 pairs of images, equvalent to around 1200 batches where batch size is 128. But due to the lack of computational power and the fact that a single 200 epoch training with 100 batches each takes about 1-2 hours, the maximum  number of batches per epoch in this implementation has been reduced to 100.\n"
      ]
    },
    {
      "metadata": {
        "id": "MvFgcL2q_ICr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "space = {\n",
        "    'l_c1' : 10 ** hp.uniform('l_c1', -4, -1),\n",
        "    'l_c2' : 10 ** hp.uniform('l_c2', -4, -1),\n",
        "    'l_c3' : 10 ** hp.uniform('l_c3', -4, -1),\n",
        "    'l_c4' : 10 ** hp.uniform('l_c4', -4, -1),\n",
        "    'l_d1' : 10 ** hp.uniform('l_d1', -4, -1),\n",
        "    'm_c1' : hp.uniform('m_c1', 0.5, 1),\n",
        "    'm_c2' : hp.uniform('m_c2', 0.5, 1),\n",
        "    'm_c3' : hp.uniform('m_c3', 0.5, 1),\n",
        "    'm_c4' : hp.uniform('m_c4', 0.5, 1),\n",
        "    'm_d1' : hp.uniform('m_d1', 0.5, 1),\n",
        "    'r_c1' : hp.uniform('r_c1', 0, 0.1),\n",
        "    'r_c2' : hp.uniform('r_c2', 0, 0.1),\n",
        "    'r_c3' : hp.uniform('r_c3', 0, 0.1),\n",
        "    'r_c4' : hp.uniform('r_c4', 0, 0.1),\n",
        "    'r_d1' : hp.uniform('r_d1', 0, 0.1),\n",
        "    'epochs' : hp.choice('epochs', [100, 150, 200]),\n",
        "    'steps_per_epoch' : hp.choice('steps_per_epoch', [25, 50, 75, 100])\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xIkDDKg_k7we",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Save Optimization state by pickling Trials object after each trial/run.**\n",
        "\n",
        "**In this implementation, 100 trials are run to find the optimum hyperparameters.**\n",
        "\n",
        "**SInce Keras models are not serializable, the models themselves are not saved. After finding the best hyperparameters, a final model is trained using them and stored as the best model.**"
      ]
    },
    {
      "metadata": {
        "id": "KJ8SwSLURKNY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def remove_models(trials):\n",
        "    for trial in trials.trials:\n",
        "      if 'result' in trial.keys() and 'model' in trial['result'].keys():\n",
        "          trial['result'].pop('model', None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOrQ3XKbVtZ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "ece46f6c-1e33-4967-c8d8-856428768b16"
      },
      "cell_type": "code",
      "source": [
        "with open(trials_path, 'rb') as f:\n",
        "    trials = pickle.load(f, encoding='latin1')\n",
        "print(len(trials))\n",
        "while(len(trials) < 100): \n",
        "\n",
        "  best = fmin(One_Shot_Model, space, algo=tpe.suggest, trials=trials, max_evals=len(trials) + 1)\n",
        " \n",
        "  remove_models(trials)\n",
        "\n",
        "  with open(trials_path, 'wb') as f:\n",
        "    pickle.dump(trials, f, -1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66\n",
            "{'epochs': 150, 'l_c1': 0.00011831856279961705, 'l_c2': 0.00010065351241952536, 'l_c3': 0.010249239651501155, 'l_c4': 0.044233809919075696, 'l_d1': 0.00767035801649971, 'm_c1': 0.6581077696847627, 'm_c2': 0.9543505394376957, 'm_c3': 0.9451833775277471, 'm_c4': 0.7119453351451809, 'm_d1': 0.8866928279235069, 'r_c1': 0.017088107427022787, 'r_c2': 0.09358188472834854, 'r_c3': 0.09498433743775812, 'r_c4': 0.09726822753670485, 'r_d1': 0.08230122244095776, 'steps_per_epoch': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xcyM9a8YwdA1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_model_path = os.path.join(one_shot_path, 'One_Shot_Model.h5')\n",
        "print(\"Best performing model chosen hyper-parameters:\")\n",
        "print(trials.best_trial)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}